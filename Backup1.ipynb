{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”» [Return to workflow](#leftoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš“ ANCHOR FOR RETURN TO WORKFLOW LINK <a name=\"leftoff\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¡ **AirBNB Dataset Review** ðŸ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âŒ Update target audience and guiding questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Who?**\n",
    ">* ðŸ¢ **AirBNB Corporate** interested in maximizing customer satisfaction to increase repeat guests and encourage new guests to stay with AirBNB hosts\n",
    ">\n",
    ">\n",
    ">* ðŸ¡**AirBNB hosts** interested in maximizing the ratings\n",
    "\n",
    "**Why?**\n",
    ">* ðŸ’° **Revenue Management:** \n",
    ">\n",
    ">\n",
    ">\n",
    ">* ðŸ¤ **Sales:**\n",
    ">\n",
    ">\n",
    ">\n",
    ">* ðŸ›Œ **Rooms Ops:**\n",
    "\n",
    ">\n",
    ">\n",
    ">\n",
    "\n",
    "**What?**\n",
    ">* ðŸ§¾ Dataset comprised of... \n",
    ">  * different features\n",
    ">  * reservation records\n",
    ">  * Source cited in Readme\n",
    "\n",
    "âŒ **How?**\n",
    ">* Which models/methods?\n",
    ">* Data prep and feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯  **Goal:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining whether or not a host location would receive a score greater than or equal to 4/5 (defined by `'review_scores_rating'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ **To-Do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- [ ] [TD1](#td1)\n",
    "- [ ] [TD2](#td2)\n",
    "- [ ] [TD3](#td3)\n",
    "- [ ] [todo4](#td4)\n",
    "- [ ] [todo5](#td5)\n",
    "- [ ] [todo6](#td6)\n",
    "- [ ] [todo7](#td7)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‚ **Imports and Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:15.599408Z",
     "start_time": "2021-08-03T20:14:13.629406Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact_manual\n",
    "import missingno\n",
    "\n",
    "## Modeling - SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB # for naive bayes model\n",
    "\n",
    "## Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.179468Z",
     "start_time": "2021-08-03T20:14:15.602407Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Personal functions\n",
    "import clf_functions.functions as cf\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport clf_functions.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Show Visualizations Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.195433Z",
     "start_time": "2021-08-03T20:14:16.181406Z"
    }
   },
   "outputs": [],
   "source": [
    "## Controlling whether or not to show visualizations\n",
    "show_visualizations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ FSDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.211408Z",
     "start_time": "2021-08-03T20:14:16.197408Z"
    }
   },
   "outputs": [],
   "source": [
    "# import fsds as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.227410Z",
     "start_time": "2021-08-03T20:14:16.214408Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fs.ihelp_menu([fs.ihelp_menu, sort_report])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“– **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.671407Z",
     "start_time": "2021-08-03T20:14:16.232416Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading data and saving to a DataFrame\n",
    "\n",
    "source = 'data/listings.csv.gz'\n",
    "\n",
    "data = pd.read_csv(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.733417Z",
     "start_time": "2021-08-03T20:14:16.673410Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Inspecting imported dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:16.748408Z",
     "start_time": "2021-08-03T20:14:16.740423Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The initial read of the dataset shows there are 74 features and 8,033 entries. A quick glance at the `.head()` gives a sample of the entries, showing that some of the features are not relevant to my analysis.\n",
    ">\n",
    "> I need to get a better idea of the statistics for the dataset, especially any missing values and the datatypes for each column. I need to pre-process this data before I can perform any modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘¨â€ðŸ’» **Interactive Investigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To increase accessibility to the data, **I include a widget to allow the user to sort through the data interactively.** I use [**Jupyter Widgets**](https://ipywidgets.readthedocs.io/en/latest/index.html) to create this interactive report.\n",
    ">\n",
    ">**To use:** select which column by which you would like to sort from the dropdown menu, then click the \"Run Interact\" button.\n",
    ">\n",
    ">***Note about 'Drop_Cols' and Cols:*** these keyword arguments are used to allow the user to drop specific columns.\n",
    ">\n",
    "> **Only click the \"Drop_Cols\" option when specifying \"Cols\"!** Otherwise it will cause an error.\n",
    ">\n",
    ">The 'Cols' dropdown menu does not affect the resulting report; the data is filtered from the report prior to displaying the results. \n",
    ">\n",
    ">I chose to include this option for flexibility and adaptability, but it does have the unintended consequence of creating another drop-down menu. Please ignore this menu, as it does not provide any additional functionality. For future work, I will disable the menu to prevent confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.097409Z",
     "start_time": "2021-08-03T20:14:16.752410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Running report on unfiltered dataset\n",
    "\n",
    "interact_manual(cf.sort_report, Sort_by=list(cf.report_df(data).columns),\n",
    "                Source=source);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.159406Z",
     "start_time": "2021-08-03T20:14:17.099408Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After reviewing my data, I see there are several features that contain irrelevant entries (URLs, source data, meta data) or values that are too complicated for simple processing (such as host and listing descriptions).\n",
    ">\n",
    "> I will drop these columns for the second report to review the remaining data for further processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.175407Z",
     "start_time": "2021-08-03T20:14:17.161409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying columns to drop\n",
    "\n",
    "drop = ['id', 'host_id', 'name', 'description', 'neighborhood_overview', 'host_name',\n",
    "        'host_about', 'host_location', 'neighbourhood', 'property_type',\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'picture_url','host_url',\n",
    "        'host_thumbnail_url','host_picture_url','calendar_last_scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.429413Z",
     "start_time": "2021-08-03T20:14:17.177409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Creating updated interactive report\n",
    "\n",
    "interact_manual(cf.sort_report, Drop_Cols = True, Cols = drop,\n",
    "                Sort_by=list(cf.report_df(data).columns), Source=source);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Interpretation:**\n",
    ">\n",
    "> The report shows that the dataset has a big problem with missing values:\n",
    ">\n",
    "> * **Empty:**\n",
    ">   * `neighbourhood_group_cleansed`\n",
    ">   * `bathrooms`\n",
    ">   * `calendar_updated`\n",
    ">\n",
    ">\n",
    "> * **Nearly empty:**\n",
    ">  * `license`\n",
    ">\n",
    ">\n",
    "> * **Missing 26-39% of data:**\n",
    ">  * `host_about`\n",
    ">  * `neighborhood_overview`\n",
    ">  * `neighbourhood`\n",
    ">  * `host_response_time`\n",
    ">  * `host_response_rate`\n",
    ">  * `review_scores_value`\n",
    ">  * `review_scores_checkin`\n",
    ">  * `review_scores_location`\n",
    ">  * `review_scores_accuracy`\n",
    ">  * `review_scores_communication`\n",
    ">  * `review_scores_cleanliness`\n",
    ">  * `host_acceptance_rate`\n",
    ">  * `reviews_per_month`\n",
    ">  * `first_review`\n",
    ">  * `review_scores_rating`\n",
    ">  * `last_review`\n",
    ">\n",
    ">---\n",
    ">\n",
    "> I will need to address these missing values before processing with the modeling. A few options include:\n",
    ">\n",
    "> * **Filling with the string \"missing\"** to indicate the value was missing.\n",
    ">    * *I would be able to treat \"missing\" as a distinct category and use it for modeling as well.*\n",
    ">\n",
    ">\n",
    "> * **Dropping the rows with missing values.**\n",
    ">    * *This may negatively impact the accuracy of my results by overfitting to the training data.*\n",
    ">\n",
    ">\n",
    "> * I could **use the `SimpleImputer` tool from SKLearn to fill the missing values** with the mean, median, or mode values for each.\n",
    ">    * *I could couple this with a `GridSearchCV` to identify the method that has the strongest positive impact on my classification metrics.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To get a better idea of the missing values, I create a visual of the values via the 'Missingno' package. This visualization package includes several options for visualizing the missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.445414Z",
     "start_time": "2021-08-03T20:14:17.431413Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visually inspecting missing values\n",
    "if show_visualizations == True:\n",
    "    missingno.bar(data, labels=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on this visualization, I see that **there is a consistent trend in missing values for review scores:** if a row is missing one review score, it seems to be missing all of them.\n",
    ">\n",
    "> Additionally, **there are many missing values for the response time, response rate, and acceptance rate.** I want to use these columns in my classification, so I will need to replace those missing values.\n",
    ">\n",
    "> After reviewing these details, **I feel more comfortable with the option of dropping those rows with missing review values.** I will drop the values as part of my overall classification process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸª“ **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Before I run any further pre-processing, I split my data into training and test sets to allow me to test my model's performance.\n",
    ">\n",
    "> **In order to split my classification target feature properly, I will convert the original values to binary values.** Since my goal is to determine whether or not a given host property will have a high score (4+), I assign all values greater-than or equal-to 4 to '1' and anything less than 4 as '0.'\n",
    ">\n",
    "> **This conversion also allows me to use the \"stratify\" parameter in my train/test split,** which will preserve the class balance when I split my data. This will be key for proper evaluation of my models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.461417Z",
     "start_time": "2021-08-03T20:14:17.447412Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using np.select to reassign target values based on conditional evaluations\n",
    "\n",
    "cond = [data['review_scores_rating'] >= 4,\n",
    "        data['review_scores_rating'] < 4\n",
    "       ]\n",
    "\n",
    "choice = [1,0]\n",
    "\n",
    "data['review_scores_rating'] = np.select(cond, choice, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.477415Z",
     "start_time": "2021-08-03T20:14:17.463409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing results to confirm only 0/1 values\n",
    "data['review_scores_rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.508443Z",
     "start_time": "2021-08-03T20:14:17.481410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating features/target for dataset\n",
    "target = 'review_scores_rating'\n",
    "\n",
    "X = data.drop(columns = target).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.523411Z",
     "start_time": "2021-08-03T20:14:17.510412Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming same number of rows\n",
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.555408Z",
     "start_time": "2021-08-03T20:14:17.525410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¼ **Data Cleaning and EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Fixing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> This dataset is missing a significant number of values for different columns. **In order to perform any modeling, I will need to address these missing values first.**\n",
    ">\n",
    "> Depending on the feature and the number of missing values per row, I will take different approaches to keep as much data as possible and in its original state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.619409Z",
     "start_time": "2021-08-03T20:14:17.558412Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping features with high percentages (25%+) of missing values\n",
    "\n",
    "drop_na_cols = []\n",
    "for col in X_train.columns:\n",
    "    if ((X_train[col].isna().sum()) / len(X_train[col])) > .25 and col != 'review_scores_rating':\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.635411Z",
     "start_time": "2021-08-03T20:14:17.622410Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Appending previous list of columns to drop (metadata, etc.)\n",
    "\n",
    "for col in drop:\n",
    "    if col not in drop_na_cols:\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.697410Z",
     "start_time": "2021-08-03T20:14:17.638410Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating new dataframe that does not include the features to drop\n",
    "X_train = X_train.drop(columns= drop_na_cols).copy()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.823408Z",
     "start_time": "2021-08-03T20:14:17.699411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Confirming dropped columns with high missing values\n",
    "cf.report_df(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.871410Z",
     "start_time": "2021-08-03T20:14:17.825409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'beds' with values for 'bedrooms'\n",
    "\n",
    "for idx in list(X_train['beds'][X_train['beds'].isna()].index):\n",
    "    if X_train['bedrooms'][idx] > 0:\n",
    "        X_train['beds'][idx] = X_train['bedrooms'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:17.966410Z",
     "start_time": "2021-08-03T20:14:17.874411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'bedrooms' with values for 'beds'\n",
    "\n",
    "for idx in list(X_train['bedrooms'][X_train['bedrooms'].isna()].index):\n",
    "    if X_train['beds'][idx] > 0:\n",
    "        X_train['bedrooms'][idx] = X_train['beds'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.075444Z",
     "start_time": "2021-08-03T20:14:17.968409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming reduction in missing values for 'beds' and 'bedrooms'\n",
    "\n",
    "rpt_clean  = cf.report_df(X_train)\n",
    "rpt_clean[rpt_clean['null_sum'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.106439Z",
     "start_time": "2021-08-03T20:14:18.080450Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking remaining missing values\n",
    "\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.153415Z",
     "start_time": "2021-08-03T20:14:18.111411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Removing rows with 6+ null values\n",
    "\n",
    "X_train = X_train[X_train.isna().sum(axis=1) < 6]\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.185411Z",
     "start_time": "2021-08-03T20:14:18.158411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.311408Z",
     "start_time": "2021-08-03T20:14:18.190412Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.report_df(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.327439Z",
     "start_time": "2021-08-03T20:14:18.313409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Resetting the index after dropping rows\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.343438Z",
     "start_time": "2021-08-03T20:14:18.329410Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(X_train) == len(X_train.index),\"\\n\")\n",
    "print(len(X_train),len(X_train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:24:46.416827Z",
     "start_time": "2021-08-02T19:24:46.402822Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> At this point, **I cleaned up most of the null values via dropping columns with 25%+ missing values and dropping rows with 6+ missing values.**\n",
    ">\n",
    ">Additionally, **I filled missing values for 'beds'/'bedrooms' by checking the missing values for each column against the values in the other for each row.** If a row had a value in one of the columns but not the other, I filled the missing value with the value from the other column.\n",
    ">\n",
    "> At this point, I addressed most of the missing values in my dataset by dropping columns and filling missing values. There are still a few columns with missing values, but I will use a SimpleImputer combined with a GridSearchCV to determine the best method by which to fill those values.\n",
    ">\n",
    "> Now I will review the remaining data and determine if there are any other issues with my data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.359440Z",
     "start_time": "2021-08-03T20:14:18.345410Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_train) == len(X_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMMENT:** What else to clean?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DONE: T/F columns to 1/0\n",
    "\n",
    "\n",
    "* DONE: 'host_since' to DT\n",
    "\n",
    "\n",
    "* DONE: 'price' -$, to float\n",
    "\n",
    "\n",
    "* DONE: 'neighbourhood_cleansed' split on \", \" and convert to binary columns, then drop host_neighbourhood\n",
    "\n",
    "\n",
    "* DONE: 'bathrooms_text' split on space, keep 1st part, convert to int\n",
    "\n",
    "\n",
    "* 'host_verifications' - single string, needs extensive work in order to MLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting True/False Columns to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.375410Z",
     "start_time": "2021-08-03T20:14:18.361411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of true/false features to convert to 1/0, respectively\n",
    "\n",
    "t_f_xf = ['host_is_superhost','host_has_profile_pic','host_identity_verified',\n",
    "          'has_availability','instant_bookable']\n",
    "t_f_xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.407411Z",
     "start_time": "2021-08-03T20:14:18.377410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting datatype to \"string\" to replace values\n",
    "\n",
    "X_train[t_f_xf] = X_train[t_f_xf].astype('str')\n",
    "X_train[t_f_xf].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.439408Z",
     "start_time": "2021-08-03T20:14:18.410411Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.471409Z",
     "start_time": "2021-08-03T20:14:18.441408Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting t/f to 1/0, respectively\n",
    "\n",
    "X_train[t_f_xf] = X_train[t_f_xf].replace({ 't' : 1, 'f' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.487409Z",
     "start_time": "2021-08-03T20:14:18.473410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.503408Z",
     "start_time": "2021-08-03T20:14:18.489408Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf] = X_train[t_f_xf].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.535410Z",
     "start_time": "2021-08-03T20:14:18.505410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Verifying results\n",
    "\n",
    "cf.report_df(X_train[t_f_xf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Price to Float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.551443Z",
     "start_time": "2021-08-03T20:14:18.537410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each value into a float for processing\n",
    "\n",
    "try:\n",
    "    X_train['price'] = X_train['price'].map(lambda price: price[1:].replace(',','')).astype('float')\n",
    "    X_train['price'][0]\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary')\n",
    "    print(f\"\\nSample value: {X_train['price'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.567436Z",
     "start_time": "2021-08-03T20:14:18.553412Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Host_Since to Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Since the 'host_since' feature is clearly a date, I will convert it to the \"datetime\" datatype. This allows me to create a separate feature for how many years of activity for each host.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.598409Z",
     "start_time": "2021-08-03T20:14:18.569409Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.629409Z",
     "start_time": "2021-08-03T20:14:18.600411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'host_since'] = pd.to_datetime(X_train.loc[:,'host_since'])\n",
    "X_train['host_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.645421Z",
     "start_time": "2021-08-03T20:14:18.631410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[\"host_since\"].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"Years_Hosting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.661410Z",
     "start_time": "2021-08-03T20:14:18.650413Z"
    }
   },
   "outputs": [],
   "source": [
    "range(len(X_train[\"host_since\"]) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.739408Z",
     "start_time": "2021-08-03T20:14:18.665419Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'] = X_train[\"host_since\"].map(lambda x: 2021- int(str(x).split(\"-\")[0]))\n",
    "X_train['years_hosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.755422Z",
     "start_time": "2021-08-03T20:14:18.742412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.771409Z",
     "start_time": "2021-08-03T20:14:18.757410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> I successfully created the new feature to represent how long each host is active (up to 2021). I will be curious to see the impact of the years of experience on the overall rating at the end of my modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathrooms_Text to Num_Bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In the raw data, the original \"bathrooms\" feature was empty and was dropped as part of processing missing data.\n",
    ">\n",
    "> **My goal is to convert the \"bathrooms_text\" feature into a new \"num_bathrooms\" feature to indicate the number of bathrooms at a host property.**\n",
    ">\n",
    "> I assume the number of bathrooms would have an impact on the rating . More bathrooms could mean more space/comfort for the guest, but could also cause an increase in price.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.803408Z",
     "start_time": "2021-08-03T20:14:18.774413Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking current dataframe contents\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.834409Z",
     "start_time": "2021-08-03T20:14:18.805413Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Checking for null values overall\n",
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.850410Z",
     "start_time": "2021-08-03T20:14:18.836410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting a selection of values from the column to understand the values\n",
    "X_train.loc[:,'bathrooms_text'][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.896423Z",
     "start_time": "2021-08-03T20:14:18.852409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the rows in which there are null values\n",
    "X_train[X_train['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.943412Z",
     "start_time": "2021-08-03T20:14:18.900420Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling null values with unique string ('Baths' not present otherwise)\n",
    "## Unique string can be used later to check for any other zero baths\n",
    "\n",
    "X_train.loc[:,'bathrooms_text'].fillna('0 Baths', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.991411Z",
     "start_time": "2021-08-03T20:14:18.946409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Verifying all null values are filled\n",
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.007409Z",
     "start_time": "2021-08-03T20:14:18.993412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.023411Z",
     "start_time": "2021-08-03T20:14:19.009411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting each list into separate strings\n",
    "X_train['num_bathrooms'] = X_train['bathrooms_text'].map(lambda x: x.split(' ')[0])\n",
    "X_train['num_bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.039409Z",
     "start_time": "2021-08-03T20:14:19.025411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting results that are phrases, not numbers\n",
    "\n",
    "replace = ['Half-bath', 'Shared', 'Private']\n",
    "\n",
    "for x in X_train['bathrooms_text']:\n",
    "    for i in replace:\n",
    "        if i in x:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **I will replace these values with the numeric value .5 as they are half-baths.** This will allow me to convert the column datatype to a float and use the column more easily in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.055409Z",
     "start_time": "2021-08-03T20:14:19.041409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Replacing string values with .5 to represent half-bathrooms\n",
    "\n",
    "replace = {'Half-bath': .5, 'Shared': .5, 'Private': .5}\n",
    "\n",
    "X_train['num_bathrooms'].replace(replace, inplace = True)\n",
    "\n",
    "X_train['num_bathrooms'] = X_train['num_bathrooms'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.071409Z",
     "start_time": "2021-08-03T20:14:19.057411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting resulting values\n",
    "\n",
    "X_train['num_bathrooms'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.103445Z",
     "start_time": "2021-08-03T20:14:19.073411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting listings with more than 10 rooms\n",
    "\n",
    "X_train[X_train['num_bathrooms'] >10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After taking a look at the locations listed above on Google Maps (using their latitude/longitude), I feel like these three listings with more than 10 bathrooms are either duplicates or incorrect values (for 50 baths).\n",
    ">\n",
    "> Due to the questionable nature of these values, I will drop these rows to prevent these outliers from impacting my results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.151418Z",
     "start_time": "2021-08-03T20:14:19.105409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting rows where 'num_bathrooms' is zero to validate data\n",
    "\n",
    "X_train[X_train['num_bathrooms'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.167413Z",
     "start_time": "2021-08-03T20:14:19.155412Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing old column post-conversion\n",
    "\n",
    "X_train = X_train.drop(columns = 'bathrooms_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.183411Z",
     "start_time": "2021-08-03T20:14:19.170414Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming removal\n",
    "\n",
    "'bathrooms_text' in X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> My review of the original bathroom text for the zero bathrooms column shows that the listings are associated with a private room. This would make sense as the listings may not include an option such as a shared bath, etc..\n",
    ">\n",
    "> Additionally I did fill 9 instances of missing values with \"0 Baths,\" which would contribute slightly to this count.\n",
    ">\n",
    "> Overall, I feel the data is valid and I will use it for my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing Neighbourhood_Cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The current values for \"neighbourhood_cleansed\" are a single string value. **I will separate each neighborhood and convert them into a binary column to represent whether or not that neighborhood is included in the listing, then drop the old column.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.198410Z",
     "start_time": "2021-08-03T20:14:19.186411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'neighbourhood_cleansed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.213410Z",
     "start_time": "2021-08-03T20:14:19.200411Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'neighbourhood_cleansed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.229409Z",
     "start_time": "2021-08-03T20:14:19.215410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the splitting between neighborhoods\n",
    "\n",
    "X_train.loc[:,'neighbourhood_cleansed'][1].split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.259407Z",
     "start_time": "2021-08-03T20:14:19.231410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting values into a list of strings for each neighborhood\n",
    "\n",
    "try:\n",
    "    X_train['neighbourhood_cleansed'] = X_train['neighbourhood_cleansed'] \\\n",
    "                                                .apply(lambda x: x.split(', '))\n",
    "    display(X_train.loc[:,'neighbourhood_cleansed'])\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary.')\n",
    "    print(f\"\\nSample value: {X_train.loc[:,'neighbourhood_cleansed'][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The following code snippet is adapted from [here](https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list#:~:text=Sparse%20solution%20(for%20Pandas%20v0.25.0%2B)) by the user [Maxu](https://stackoverflow.com/users/5741205/maxu).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.305411Z",
     "start_time": "2021-08-03T20:14:19.261411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each neighborhood into a binary column and dropping old column\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "try:\n",
    "    X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop('neighbourhood_cleansed')),\n",
    "                              columns=mlb.classes_,index=X_train.index))\n",
    "except Exception:\n",
    "        print('\\nValues are already processed and saved. No changes necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.398421Z",
     "start_time": "2021-08-03T20:14:19.308412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting results\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After using the MultiLabelBinarizer, I successfully added a column for each neighborhood, indicating whether or not that neighborhood was included in the listing.\n",
    ">\n",
    "> This enables me to use the presence/absence of a  neighborhood as a category in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host_Verifications to Binary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.414415Z",
     "start_time": "2021-08-03T20:14:19.403416Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['host_verifications'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> For the \"host_verifications\" and \"amenities\" features, the values are a single string with several items within the string.\n",
    ">\n",
    "> It is somewhat similar to the \"neighborhoods_cleaned\" feature in the sense that I will need to filter out the individual items from the string. However, there is an added complication as I need to remove the brackets and quotations from the strings.\n",
    ">\n",
    "> Once I filter out the items, I will be able to use the MultiLabelBinarizer again to create more categories for each amenity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.429411Z",
     "start_time": "2021-08-03T20:14:19.417417Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the splitting between items\n",
    "\n",
    "X_train.loc[:,'host_verifications'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.554409Z",
     "start_time": "2021-08-03T20:14:19.431411Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in ['host_verifications', 'amenities']:\n",
    "    X_train[x] = X_train[x].str.replace('[', '')\n",
    "    X_train[x] = X_train[x].str.replace(']', '')\n",
    "    X_train[x] = X_train[x].str.replace(\"'\", '')\n",
    "    X_train[x] = X_train[x].str.replace('\"', '')\n",
    "    X_train[x] = X_train[x].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.727448Z",
     "start_time": "2021-08-03T20:14:19.556410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each value into a binary column and dropping old column\n",
    "\n",
    "mlb2 = MultiLabelBinarizer()\n",
    "    \n",
    "X_train = X_train.join(pd.DataFrame(mlb2.fit_transform(X_train.pop('host_verifications')),\n",
    "                                  columns=mlb2.classes_,index=X_train.index))\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> At this point, I successfully processed the 'host_verification' feature into distinct categories for modeling.\n",
    ">\n",
    "> In the future, I may attempt to do the same for the 'amenities' feature, but I don't want to create too many columns before my initial modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Remaining Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.743410Z",
     "start_time": "2021-08-03T20:14:19.729411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.dtypes[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## âŒ ERROR âŒ Binarizing Room_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "> **Can't get MLB/OHE to work for individual property types.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.759410Z",
     "start_time": "2021-08-03T20:14:19.745411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['room_type'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.775410Z",
     "start_time": "2021-08-03T20:14:19.761411Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train['room_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.791410Z",
     "start_time": "2021-08-03T20:14:19.777409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['room_type'] = X_train['room_type'].replace('Entire home/apt', 'Home/Apt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.807409Z",
     "start_time": "2021-08-03T20:14:19.793410Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['room_type'] = X_train['room_type'].map(lambda x: x.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.823410Z",
     "start_time": "2021-08-03T20:14:19.809412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['room_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.839410Z",
     "start_time": "2021-08-03T20:14:19.831412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# X_train_ohe = ohe.fit_transform([X_train['room_type']])\n",
    "# X_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.855409Z",
     "start_time": "2021-08-03T20:14:19.842413Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_train_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## âŒ ERROR âŒ Converting Amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "> same issue as w/ room type\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.871410Z",
     "start_time": "2021-08-03T20:14:19.857424Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for x in ['host_verifications', 'amenities']:\n",
    "#     print(X_train[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.887410Z",
     "start_time": "2021-08-03T20:14:19.874411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['amenities'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.903416Z",
     "start_time": "2021-08-03T20:14:19.889410Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for x in ['host_verifications', 'amenities']:\n",
    "#     X_train[x] = X_train[x].str.replace('and', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.919411Z",
     "start_time": "2021-08-03T20:14:19.907416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Converting each value into a binary column and dropping old column\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "    \n",
    "# X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop('amenities')),\n",
    "#                                   columns=mlb.classes_,index=X_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.935414Z",
     "start_time": "2021-08-03T20:14:19.922413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train.loc[:,'host_verifications'] = X_train.loc[:,'host_verifications'].str.replace('[', '')\n",
    "# X_train.loc[:,'host_verifications'] = X_train.loc[:,'host_verifications'].str.replace(']', '')\n",
    "# X_train.loc[:,'host_verifications'] = X_train.loc[:,'host_verifications'].str.replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.951414Z",
     "start_time": "2021-08-03T20:14:19.938414Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train.loc[:,'host_verifications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.967414Z",
     "start_time": "2021-08-03T20:14:19.955412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['amenities'] = X_train['amenities'].str.replace('[', '')\n",
    "# X_train['amenities'] = X_train['amenities'].str.replace(']', '')\n",
    "# X_train['amenities'] = X_train['amenities'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.983411Z",
     "start_time": "2021-08-03T20:14:19.970415Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train['amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.998411Z",
     "start_time": "2021-08-03T20:14:19.986411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['amenities'] = X_train['amenities'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.014411Z",
     "start_time": "2021-08-03T20:14:20.001413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['amenities'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.029410Z",
     "start_time": "2021-08-03T20:14:20.017411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['host_verifications'] = X_train['host_verifications'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.045409Z",
     "start_time": "2021-08-03T20:14:20.032411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_train['host_verifications'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.061409Z",
     "start_time": "2021-08-03T20:14:20.048411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_col(X_train, list_cols):\n",
    "#     '''For a given list of column names, separates each string value in the\n",
    "#     column by the comma/space pattern to return new strings of single values.\n",
    "    \n",
    "#     Then, instantiates a MultiLabelBinarizer to create new columns for each \n",
    "#     new string to indicate the presence or absence of that string in the \n",
    "#     original column.'''\n",
    "    \n",
    "# #     mlb = MultiLabelBinarizer()\n",
    "    \n",
    "#     for x in list_cols:\n",
    "#         try:\n",
    "#             X_train[x] = X_train[x].apply(lambda x: x.split(', '))\n",
    "#             print(f'Successfully split values in column \"{x}\"')\n",
    "            \n",
    "#         except Exception:\n",
    "#             print('\\nValues are already processed and saved.')\n",
    "#             print(f\"\\nSample value: {X_train.loc[:,x][3]}\")\n",
    "            \n",
    "# #         try:\n",
    "# #             X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop(x)),\n",
    "# #                                       columns=mlb.classes_,index=X_train.index))\n",
    "# #         except Exception:\n",
    "# #                 print('\\nValues are already processed and saved.')\n",
    "                \n",
    "#     return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.077410Z",
     "start_time": "2021-08-03T20:14:20.063412Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# binarize_cols = ['host_verifications', 'amenities'] \n",
    "\n",
    "# convert_to_col(X_train, binarize_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.092410Z",
     "start_time": "2021-08-03T20:14:20.079411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Converting each value into a binary column and dropping old column\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "    \n",
    "# X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop('amenities')),\n",
    "#                                   columns=mlb.classes_,index=X_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.107411Z",
     "start_time": "2021-08-03T20:14:20.095411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # mlb = MultiLabelBinarizer()\n",
    "    \n",
    "# X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop('amenities')),\n",
    "#                                   columns=mlb.classes_,index=X_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.123413Z",
     "start_time": "2021-08-03T20:14:20.110413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Converting values into a list of strings for each neighborhood\n",
    "\n",
    "# try:\n",
    "#     X_train['host_verifications'] = X_train['host_verifications'] \\\n",
    "#                                                 .apply(lambda x: x.split(', '))\n",
    "#     display(X_train.loc[:,'host_verifications'])\n",
    "# except Exception:\n",
    "#     print('\\nValues are already processed and saved. No changes necessary.')\n",
    "#     print(f\"\\nSample value: {X_train.loc[:,'host_verifications'][3]}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.139411Z",
     "start_time": "2021-08-03T20:14:20.126414Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Inspecting results\n",
    "\n",
    "# X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.155411Z",
     "start_time": "2021-08-03T20:14:20.141412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test3 = X_train['host_verifications'][0]\n",
    "# test3[1:-1].replace('\"', \"'\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:20.171409Z",
     "start_time": "2021-08-03T20:14:20.157411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # X_train['Tags'] = X_train.Tags.apply(lambda x: x[1:-1].split(','))\n",
    "\n",
    "# X_train['host_verifications'].apply(lambda x: x.split(','))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Pipeline Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:15:12.175880Z",
     "start_time": "2021-08-03T20:15:12.088879Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:17:41.883341Z",
     "start_time": "2021-08-03T20:17:41.856345Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns = ['host_since', 'host_neighbourhood', 'amenities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:20:16.541828Z",
     "start_time": "2021-08-03T20:20:16.134829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:26:38.562228Z",
     "start_time": "2021-08-03T20:26:38.537234Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transforming X_test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting True/False Columns to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.375410Z",
     "start_time": "2021-08-03T20:14:18.361411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of true/false features to convert to 1/0, respectively\n",
    "\n",
    "t_f_xf = ['host_is_superhost','host_has_profile_pic','host_identity_verified',\n",
    "          'has_availability','instant_bookable']\n",
    "t_f_xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.407411Z",
     "start_time": "2021-08-03T20:14:18.377410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting datatype to \"string\" to replace values\n",
    "\n",
    "X_train[t_f_xf] = X_train[t_f_xf].astype('str')\n",
    "X_train[t_f_xf].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.439408Z",
     "start_time": "2021-08-03T20:14:18.410411Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.471409Z",
     "start_time": "2021-08-03T20:14:18.441408Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting t/f to 1/0, respectively\n",
    "\n",
    "X_train[t_f_xf] = X_train[t_f_xf].replace({ 't' : 1, 'f' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.487409Z",
     "start_time": "2021-08-03T20:14:18.473410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.503408Z",
     "start_time": "2021-08-03T20:14:18.489408Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[t_f_xf] = X_train[t_f_xf].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.535410Z",
     "start_time": "2021-08-03T20:14:18.505410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Verifying results\n",
    "\n",
    "cf.report_df(X_train[t_f_xf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Price to Float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.551443Z",
     "start_time": "2021-08-03T20:14:18.537410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each value into a float for processing\n",
    "\n",
    "try:\n",
    "    X_train['price'] = X_train['price'].map(lambda price: price[1:].replace(',','')).astype('float')\n",
    "    X_train['price'][0]\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary')\n",
    "    print(f\"\\nSample value: {X_train['price'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.567436Z",
     "start_time": "2021-08-03T20:14:18.553412Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Host_Since to Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Since the 'host_since' feature is clearly a date, I will convert it to the \"datetime\" datatype. This allows me to create a separate feature for how many years of activity for each host.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.598409Z",
     "start_time": "2021-08-03T20:14:18.569409Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.629409Z",
     "start_time": "2021-08-03T20:14:18.600411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'host_since'] = pd.to_datetime(X_train.loc[:,'host_since'])\n",
    "X_train['host_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.645421Z",
     "start_time": "2021-08-03T20:14:18.631410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[\"host_since\"].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"Years_Hosting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.661410Z",
     "start_time": "2021-08-03T20:14:18.650413Z"
    }
   },
   "outputs": [],
   "source": [
    "range(len(X_train[\"host_since\"]) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.739408Z",
     "start_time": "2021-08-03T20:14:18.665419Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'] = X_train[\"host_since\"].map(lambda x: 2021- int(str(x).split(\"-\")[0]))\n",
    "X_train['years_hosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.755422Z",
     "start_time": "2021-08-03T20:14:18.742412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.771409Z",
     "start_time": "2021-08-03T20:14:18.757410Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['years_hosting'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> I successfully created the new feature to represent how long each host is active (up to 2021). I will be curious to see the impact of the years of experience on the overall rating at the end of my modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathrooms_Text to Num_Bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In the raw data, the original \"bathrooms\" feature was empty and was dropped as part of processing missing data.\n",
    ">\n",
    "> **My goal is to convert the \"bathrooms_text\" feature into a new \"num_bathrooms\" feature to indicate the number of bathrooms at a host property.**\n",
    ">\n",
    "> I assume the number of bathrooms would have an impact on the rating . More bathrooms could mean more space/comfort for the guest, but could also cause an increase in price.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.803408Z",
     "start_time": "2021-08-03T20:14:18.774413Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking current dataframe contents\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.834409Z",
     "start_time": "2021-08-03T20:14:18.805413Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Checking for null values overall\n",
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.850410Z",
     "start_time": "2021-08-03T20:14:18.836410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting a selection of values from the column to understand the values\n",
    "X_train.loc[:,'bathrooms_text'][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.896423Z",
     "start_time": "2021-08-03T20:14:18.852409Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the rows in which there are null values\n",
    "X_train[X_train['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.943412Z",
     "start_time": "2021-08-03T20:14:18.900420Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling null values with unique string ('Baths' not present otherwise)\n",
    "## Unique string can be used later to check for any other zero baths\n",
    "\n",
    "X_train.loc[:,'bathrooms_text'].fillna('0 Baths', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:18.991411Z",
     "start_time": "2021-08-03T20:14:18.946409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Verifying all null values are filled\n",
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.007409Z",
     "start_time": "2021-08-03T20:14:18.993412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.023411Z",
     "start_time": "2021-08-03T20:14:19.009411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting each list into separate strings\n",
    "X_train['num_bathrooms'] = X_train['bathrooms_text'].map(lambda x: x.split(' ')[0])\n",
    "X_train['num_bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.039409Z",
     "start_time": "2021-08-03T20:14:19.025411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting results that are phrases, not numbers\n",
    "\n",
    "replace = ['Half-bath', 'Shared', 'Private']\n",
    "\n",
    "for x in X_train['bathrooms_text']:\n",
    "    for i in replace:\n",
    "        if i in x:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **I will replace these values with the numeric value .5 as they are half-baths.** This will allow me to convert the column datatype to a float and use the column more easily in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.055409Z",
     "start_time": "2021-08-03T20:14:19.041409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Replacing string values with .5 to represent half-bathrooms\n",
    "\n",
    "replace = {'Half-bath': .5, 'Shared': .5, 'Private': .5}\n",
    "\n",
    "X_train['num_bathrooms'].replace(replace, inplace = True)\n",
    "\n",
    "X_train['num_bathrooms'] = X_train['num_bathrooms'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.071409Z",
     "start_time": "2021-08-03T20:14:19.057411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting resulting values\n",
    "\n",
    "X_train['num_bathrooms'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.103445Z",
     "start_time": "2021-08-03T20:14:19.073411Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting listings with more than 10 rooms\n",
    "\n",
    "X_train[X_train['num_bathrooms'] >10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After taking a look at the locations listed above on Google Maps (using their latitude/longitude), I feel like these three listings with more than 10 bathrooms are either duplicates or incorrect values (for 50 baths).\n",
    ">\n",
    "> Due to the questionable nature of these values, I will drop these rows to prevent these outliers from impacting my results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.151418Z",
     "start_time": "2021-08-03T20:14:19.105409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting rows where 'num_bathrooms' is zero to validate data\n",
    "\n",
    "X_train[X_train['num_bathrooms'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.167413Z",
     "start_time": "2021-08-03T20:14:19.155412Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing old column post-conversion\n",
    "\n",
    "X_train = X_train.drop(columns = 'bathrooms_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.183411Z",
     "start_time": "2021-08-03T20:14:19.170414Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming removal\n",
    "\n",
    "'bathrooms_text' in X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> My review of the original bathroom text for the zero bathrooms column shows that the listings are associated with a private room. This would make sense as the listings may not include an option such as a shared bath, etc..\n",
    ">\n",
    "> Additionally I did fill 9 instances of missing values with \"0 Baths,\" which would contribute slightly to this count.\n",
    ">\n",
    "> Overall, I feel the data is valid and I will use it for my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing Neighbourhood_Cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The current values for \"neighbourhood_cleansed\" are a single string value. **I will separate each neighborhood and convert them into a binary column to represent whether or not that neighborhood is included in the listing, then drop the old column.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.198410Z",
     "start_time": "2021-08-03T20:14:19.186411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'neighbourhood_cleansed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.213410Z",
     "start_time": "2021-08-03T20:14:19.200411Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,'neighbourhood_cleansed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.229409Z",
     "start_time": "2021-08-03T20:14:19.215410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the splitting between neighborhoods\n",
    "\n",
    "X_train.loc[:,'neighbourhood_cleansed'][1].split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.259407Z",
     "start_time": "2021-08-03T20:14:19.231410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting values into a list of strings for each neighborhood\n",
    "\n",
    "try:\n",
    "    X_train['neighbourhood_cleansed'] = X_train['neighbourhood_cleansed'] \\\n",
    "                                                .apply(lambda x: x.split(', '))\n",
    "    display(X_train.loc[:,'neighbourhood_cleansed'])\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary.')\n",
    "    print(f\"\\nSample value: {X_train.loc[:,'neighbourhood_cleansed'][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The following code snippet is adapted from [here](https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list#:~:text=Sparse%20solution%20(for%20Pandas%20v0.25.0%2B)) by the user [Maxu](https://stackoverflow.com/users/5741205/maxu).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.305411Z",
     "start_time": "2021-08-03T20:14:19.261411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each neighborhood into a binary column and dropping old column\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "try:\n",
    "    X_train = X_train.join(pd.DataFrame(mlb.fit_transform(X_train.pop('neighbourhood_cleansed')),\n",
    "                              columns=mlb.classes_,index=X_train.index))\n",
    "except Exception:\n",
    "        print('\\nValues are already processed and saved. No changes necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.398421Z",
     "start_time": "2021-08-03T20:14:19.308412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting results\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After using the MultiLabelBinarizer, I successfully added a column for each neighborhood, indicating whether or not that neighborhood was included in the listing.\n",
    ">\n",
    "> This enables me to use the presence/absence of a  neighborhood as a category in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host_Verifications to Binary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.414415Z",
     "start_time": "2021-08-03T20:14:19.403416Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['host_verifications'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> For the \"host_verifications\" and \"amenities\" features, the values are a single string with several items within the string.\n",
    ">\n",
    "> It is somewhat similar to the \"neighborhoods_cleaned\" feature in the sense that I will need to filter out the individual items from the string. However, there is an added complication as I need to remove the brackets and quotations from the strings.\n",
    ">\n",
    "> Once I filter out the items, I will be able to use the MultiLabelBinarizer again to create more categories for each amenity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.429411Z",
     "start_time": "2021-08-03T20:14:19.417417Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the splitting between items\n",
    "\n",
    "X_train.loc[:,'host_verifications'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.554409Z",
     "start_time": "2021-08-03T20:14:19.431411Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in ['host_verifications', 'amenities']:\n",
    "    X_train[x] = X_train[x].str.replace('[', '')\n",
    "    X_train[x] = X_train[x].str.replace(']', '')\n",
    "    X_train[x] = X_train[x].str.replace(\"'\", '')\n",
    "    X_train[x] = X_train[x].str.replace('\"', '')\n",
    "    X_train[x] = X_train[x].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.727448Z",
     "start_time": "2021-08-03T20:14:19.556410Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each value into a binary column and dropping old column\n",
    "\n",
    "mlb2 = MultiLabelBinarizer()\n",
    "    \n",
    "X_train = X_train.join(pd.DataFrame(mlb2.fit_transform(X_train.pop('host_verifications')),\n",
    "                                  columns=mlb2.classes_,index=X_train.index))\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> At this point, I successfully processed the 'host_verification' feature into distinct categories for modeling.\n",
    ">\n",
    "> In the future, I may attempt to do the same for the 'amenities' feature, but I don't want to create too many columns before my initial modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Remaining Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:14:19.743410Z",
     "start_time": "2021-08-03T20:14:19.729411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.dtypes[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¿ **Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:34:05.452883Z",
     "start_time": "2021-08-03T20:34:05.436849Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = X_train.select_dtypes(include=[int, float]).columns.to_list()\n",
    "# num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:34:05.856360Z",
     "start_time": "2021-08-03T20:34:05.850361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['room_type']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:34:06.391793Z",
     "start_time": "2021-08-03T20:34:06.380800Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[cat_cols] = X_train[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:34:06.919590Z",
     "start_time": "2021-08-03T20:34:06.910594Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_test[cat_cols] = X_test[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:39:26.037032Z",
     "start_time": "2021-08-03T20:39:25.937037Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "# Filling missing values in \"Beds\" and \"Bedrooms\"\n",
    "miss_num_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Filling missing values in \"Beds\" and \"Bedrooms\"\n",
    "miss_cat_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "## Encoding categoricals - handling errors to prevent issues w/ test set\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('imputer', miss_cat_transformer),\n",
    "                      ('ohe', categorical_transformer)])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('imputer', miss_num_transformer),\n",
    "                           ('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer and including all transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('nums', num_pipe, num_cols),\n",
    "                  ('cats', cat_pipe, cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:39:26.540725Z",
     "start_time": "2021-08-03T20:39:26.418725Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cats'].named_steps['ohe'].get_feature_names(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:39:51.284171Z",
     "start_time": "2021-08-03T20:39:51.191173Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "## Fit and transform the data via the ColumnTransformer\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "## Transforming the test set and saving\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process classification model - i.e. Logreg, KNN, DecisionTrees, etc.\n",
    "* Evaluate results\n",
    "* Determine if I need to redo pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¿ Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env-bmc)",
   "language": "python",
   "name": "learn-env-bmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.003px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
